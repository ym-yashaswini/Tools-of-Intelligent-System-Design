{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.5 64-bit"
  },
  "metadata": {
   "interpreter": {
    "hash": "ecda20238df2ed8e9d74a8333869d6ab36a80806eec0e5f6f1311a96726d7d59"
   }
  },
  "interpreter": {
   "hash": "ecda20238df2ed8e9d74a8333869d6ab36a80806eec0e5f6f1311a96726d7d59"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import genfromtxt\n",
    "np.random.seed(0)\n",
    "import pandas as pd\n",
    "from math import exp\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read dataset\n",
    "X = genfromtxt(\"train_data.csv\", delimiter=',')\n",
    "y = genfromtxt(\"train_labels.csv\", delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Number of samples in each class\n [5923. 6742. 5958. 6131.]\n"
     ]
    }
   ],
   "source": [
    "#checking the balance the dataset\n",
    "print('Number of samples in each class\\n', y.sum(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting the data into training and validation set in a ratio of 8:2\n",
    "#Created validation set to test the model on unknown data\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Number of samples in each class in y_train [4760. 5382. 4770. 4891.]\n",
      "Number of samples in each class in y_val [1163. 1360. 1188. 1240.]\n"
     ]
    }
   ],
   "source": [
    "#checking the balance the dataset\n",
    "print('Number of samples in each class in y_train', y_train.sum(axis=0))\n",
    "print('Number of samples in each class in y_val', y_val.sum(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding out the number of samples and features in train dataset\n",
    "(samples,features) = X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#declaring the number of hidden layer neurons, classes, learning rate and number of epoch for the training\n",
    "\n",
    "hiddenlyr_nodes = 35 \n",
    "num_classes = 4\n",
    "learning_rate = 0.0001\n",
    "num_epoch  = 100"
   ]
  },
  {
   "source": [
    "$Sigmoid Function = \\frac{1}{1+e^x}$\n",
    "\n",
    "S(x)\t=\tsigmoid function\n",
    "e\t=\tEuler's number"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sigmoid function\n",
    "def sigmoid_func(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "# derivative of sigmoid function\n",
    "def sigmoid_drv(x):\n",
    "    return sigmoid_func(x) * (1 - sigmoid_func(x))"
   ]
  },
  {
   "source": [
    "Sigmoid Fuunction\n",
    "$\\sigma(\\vec{z})_{i}=\\frac{e^{z_{i}}}{\\sum_{j=1}^{K} e^{z_{j}}}$\n",
    "\n",
    "$\\sigma$ = softmax\n",
    "$\\vec{z}$ =\tinput vector\n",
    "$e^{z_{i}}$\t=\tstandard exponential function for input vector\n",
    "K\t=\tnumber of classes in the multi-class classifier\n",
    "$e^{z_{j}}$\t=\tstandard exponential function for output vector\n",
    "$e^{z_{j}}$\t=\tstandard exponential function for output vector\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# softmax function\n",
    "def softmax_func(x):\n",
    "    val = np.exp(x) / np.exp(x).sum(axis=1, keepdims=True)      \n",
    "    return val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert predicted data into one hot encoding\n",
    "def one_hot_enc(x):\n",
    "    for i in range(0,len(x)):\n",
    "      x[i,x[i,:].argmax()]=1\n",
    "    out = (x == 1).astype(float)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predicting the accuracy of the model\n",
    "def accuracy(y_true, y_pred):\n",
    "    if not (len(y_true) == len(y_pred)):\n",
    "        print('Size of predicted and true labels not equal.')\n",
    "        return 0.0\n",
    "\n",
    "    corr = 0\n",
    "    for i in range(0,len(y_true)):\n",
    "        corr += 1 if (y_true[i] == y_pred[i]).all() else 0\n",
    "\n",
    "    return corr/len(y_true)\n",
    "\n",
    "# feed forward function\n",
    "def fwd(inp_data, wt_hidlyr, bias_hidlyr,wt_outlyr,bias_outlyr):\n",
    "    net_hidden = np.dot(inp_data, wt_hidlyr) + bias_hidlyr\n",
    "    act_hidden = sigmoid_func(net_hidden)\n",
    "    net_output = np.dot(act_hidden, wt_outlyr) + bias_outlyr\n",
    "    act_output = softmax_func(net_output)\n",
    "    return act_output, act_hidden, net_hidden\n",
    "\n",
    "# backpropagation function\n",
    "def bkd(X_train, y_train, net_hidden, act_hidden, weight_output, act_output):\n",
    "    cf_netHid = act_output - y_train \n",
    "    grad_bias_out = cf_netHid\n",
    "    grad_wt_out = np.dot(act_hidden.T, cf_netHid)\n",
    "    cf_actHid = np.dot(cf_netHid, weight_output.T)\n",
    "    grad_wt_hid = np.dot(X_train.T, sigmoid_drv(net_hidden) * cf_actHid)\n",
    "    grad_bias_hid = cf_actHid * sigmoid_drv(net_hidden)\n",
    "    return grad_wt_out, grad_bias_out, grad_wt_hid, grad_bias_hid\n",
    "\n",
    "# Updating Weight\n",
    "def update_weight(weight, cost):\n",
    "    if cost.shape == (features, hiddenlyr_nodes) or cost.shape == (hiddenlyr_nodes, num_classes):\n",
    "        weight = weight - learning_rate * cost\n",
    "    elif cost.shape == (samples, hiddenlyr_nodes) or cost.shape == (samples, num_classes):\n",
    "        weight = weight - learning_rate * cost.sum(axis=0)\n",
    "    return weight\n"
   ]
  },
  {
   "source": [
    "### Loss Function \n",
    "$MSE = \\frac{1}{n} + \\sum \\limits _{i=1} ^{n} (Y_{i}-\\hat{Y}_{i})^2 $\n",
    "\n",
    "MSE\t=\tmean squared error\n",
    "n\t=\tnumber of data points\n",
    "$Y_{i}$ =\tobserved values\n",
    "$\\hat{Y}_{i}$ = predicted values\n",
    "\n",
    "#### The MSE is great for ensuring that our trained model has no outlier predictions with huge errors, since the MSE puts larger weight on theses errors due to the squaring part of the function."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_ent(original_label,predict_label):\n",
    "    mse = np.square(original_label - predict_label)\n",
    "    val = np.mean(mse)\n",
    "    return val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weight initialization\n",
    "weight_hidden = np.random.randn(features, hiddenlyr_nodes)\n",
    "bias_hidden = np.random.randn(hiddenlyr_nodes)\n",
    "weight_output = np.random.randn(hiddenlyr_nodes, num_classes)\n",
    "bias_output = np.random.randn(num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "epoch =  10     Loss function value:  0.07782015133535355 accuracy =  0.8123611391638053\n",
      "epoch =  20     Loss function value:  0.038782371286173976 accuracy =  0.8889113310442335\n",
      "epoch =  30     Loss function value:  0.0327373254722133 accuracy =  0.9030498889113311\n",
      "epoch =  40     Loss function value:  0.029280660098058603 accuracy =  0.9115330236315896\n",
      "epoch =  50     Loss function value:  0.026876144030628303 accuracy =  0.918400323167037\n",
      "epoch =  60     Loss function value:  0.025069403111138894 accuracy =  0.9238537669157746\n",
      "epoch =  70     Loss function value:  0.023643780827894978 accuracy =  0.9284992930721067\n",
      "epoch =  80     Loss function value:  0.022476938312714093 accuracy =  0.9325388810341345\n",
      "epoch =  90     Loss function value:  0.021493507960131226 accuracy =  0.9359725308018582\n",
      "epoch =  100     Loss function value:  0.02064508910771253 accuracy =  0.9375883659866694\n"
     ]
    }
   ],
   "source": [
    "error_per_epoch = list()\n",
    "epoch = 0\n",
    "while epoch < num_epoch:\n",
    "  epoch+=1\n",
    "  #forward propagation\n",
    "  act_output, act_hidden, net_hidden = fwd(X_train, weight_hidden, bias_hidden, weight_output, bias_output)\n",
    "  #backward propagation\n",
    "  cost_wo, cost_bo, cost_wh, cost_bh = bkd(X_train, y_train, net_hidden, act_hidden, weight_output, act_output)\n",
    "  #weight updating\n",
    "  weight_hidden = update_weight(weight_hidden, cost_wh)\n",
    "  bias_hidden = update_weight(bias_hidden, cost_bh)\n",
    "  weight_output = update_weight(weight_output, cost_wo)\n",
    "  bias_output = update_weight(bias_output, cost_bo)\n",
    "  \n",
    "  cal_loss = cross_ent(y_train,act_output)\n",
    "  error_per_epoch.append(cal_loss)\n",
    "  \n",
    "  y_pred, _, _ = fwd(X_val, weight_hidden, bias_hidden, weight_output, bias_output)\n",
    "  # One hot encoding the prediction\n",
    "  y_pred_enc = one_hot_enc(y_pred)\n",
    "  # calculating the accuracy\n",
    "  ACC = accuracy(y_val,y_pred_enc)\n",
    "  if epoch%10==0:\n",
    "    print('epoch = ',epoch,'   ','Loss function value: ', cal_loss,'accuracy = ',ACC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[1.13965178e-01, 6.95177561e-05, 1.00000000e+00, 2.17209706e-01],\n",
       "       [3.12621456e-05, 1.00000000e+00, 6.67724574e-03, 3.57372858e-03],\n",
       "       [8.78363481e-05, 1.00000000e+00, 9.06675912e-03, 1.86532339e-02],\n",
       "       ...,\n",
       "       [1.00000000e+00, 2.02731193e-06, 2.90643006e-03, 4.86376825e-05],\n",
       "       [2.91785129e-04, 3.58194542e-02, 1.00000000e+00, 4.25361585e-03],\n",
       "       [5.55190771e-03, 1.48629551e-05, 1.00000000e+00, 5.97542651e-04]])"
      ]
     },
     "metadata": {},
     "execution_count": 28
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "accuracy on the validation set is  93.75883659866695\n"
     ]
    }
   ],
   "source": [
    "ACC= accuracy(y_val,y_pred_enc)\n",
    "print('accuracy on the validation set is ', ACC*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('Updated Weights/weight_hidden.npy', weight_hidden)\n",
    "np.save('Updated Weights/bias_hidden.npy', bias_hidden)\n",
    "np.save('Updated Weights/weight_output.npy', weight_output)\n",
    "np.save('Updated Weights/bias_output.npy', bias_output)"
   ]
  }
 ]
}